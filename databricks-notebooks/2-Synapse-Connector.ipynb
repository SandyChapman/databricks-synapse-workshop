{"cells":[{"cell_type":"markdown","source":["## Part 1: The Databricks Synapse Connector\n","\n","Pre-reqs:\n","* Storage account (ADLS Gen 2)\n","* Synapse Dedicate SQL Pool\n","* Databricks Service Principal\n","* Secret scope with service principal secret, sql password, and storage account key\n","\n","Limitations:\n","* Must connect to a sql pool (on-demand is not supported). Attempts to connect to an on-demand endpoint will result in an error.\n","* Although the Synapse connector can be used for interactive queries against your DW, it is more suited to ETL as each query execution can extract large amounts of data to blob storage. Parquet is the recommended format in such cases.\n","\n","Overview:\n","\n","1. Setup\n","  * Access secrets\n","  * Configure variables\n","  * Test storage account access\n","2. Write to Synapse\n","  * Create a dataframe\n","  * Use Synapse Spark connector to write to synapse\n","3. Read from Synapse\n","  * Use Synapse Spark connector to read written data to a new dataframe\n","4. Stream data to Synapse\n","  * TODO"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"415dfc6d-b205-4194-b4d7-83ea18499d77"}}},{"cell_type":"code","source":["from types import SimpleNamespace\n","\n","secret_scope = 'dbw-syn-lab'\n","secrets = SimpleNamespace(\n","  sp_secret = dbutils.secrets.get(secret_scope, 'dbw-syn-lab-sp-secret'),\n","  sa_secret = dbutils.secrets.get(secret_scope, 'dbw-syn-lab-sa-secret'),\n","  sql_pw = dbutils.secrets.get(secret_scope, 'dbw-syn-lab-sql-pw')\n",")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Pull Secrets from Secret Scope","showTitle":true,"inputWidgets":{},"nuid":"05dd1491-b69a-4f83-aadc-171084fe499e"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["The Azure Synapse connector uses three types of network connections:\n","\n","* Spark driver to Azure Synapse\n","* Spark driver and executors to Azure storage account\n","* Azure Synapse to Azure storage account\n","\n","```\n","                                 ┌─────────┐\n","      ┌─────────────────────────>│ STORAGE │<────────────────────────┐\n","      │   Storage acc key /      │ ACCOUNT │  Storage acc key /      │\n","      │   Managed Service ID /   └─────────┘  OAuth 2.0 /            │\n","      │                               │                              │\n","      │                               │                              │\n","      │                               │ Storage acc key /            │\n","      │                               │ OAuth 2.0 /                  │\n","      │                               │                              │\n","      v                               v                       ┌──────v────┐\n","┌──────────┐                      ┌──────────┐                │┌──────────┴┐\n","│ Synapse  │                      │  Spark   │                ││ Spark     │\n","│ Analytics│<────────────────────>│  Driver  │<───────────────>│ Executors │\n","└──────────┘  JDBC with           └──────────┘    Configured   └───────────┘\n","              username & password /                in Spark\n","```\n","\n","It should be noted that use of Blob storage can only used the Storage Account Key, whereas ADLS Gen 2 can optionally use OAuth 2.0 instead."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6e5d163c-504c-49cc-9447-0b63d4115189"}}},{"cell_type":"code","source":["# Application ID corresponds the App Registration / Service Principal used by Databricks\n","app_id = '4b309858-a987-4d5a-9a11-a84116790317'\n","\n","# Directory ID is the tenant this databricks workspace belongs to\n","directory_id = '6871727a-5747-424a-b9d4-39a621930267'\n","\n","# Defining the service principal credentials for the Azure storage account\n","spark.conf.set(\"fs.azure.account.auth.type\", \"OAuth\")\n","spark.conf.set(\"fs.azure.account.oauth.provider.type\",  \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n","spark.conf.set(\"fs.azure.account.oauth2.client.id\", app_id)\n","spark.conf.set(\"fs.azure.account.oauth2.client.secret\", secrets.sp_secret)\n","spark.conf.set(\"fs.azure.account.oauth2.client.endpoint\", f\"https://login.microsoftonline.com/{directory_id}/oauth2/token\")\n","\n","# Defining a separate set of service principal credentials for Azure Synapse Analytics (If not defined, the connector will use the Azure storage account credentials)\n","spark.conf.set(\"spark.databricks.sqldw.jdbc.service.principal.client.id\", app_id)\n","spark.conf.set(\"spark.databricks.sqldw.jdbc.service.principal.client.secret\", secrets.sp_secret)\n","\n","# Setup the storage account key\n","storage_account_name = 'strdbwsynworkshop'\n","container_name = 'synstorage'\n","spark.conf.set(f\"fs.azure.account.key.{storage_account_name}.dfs.core.windows.net\", secrets.sa_secret)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Setup variables and spark configuration","showTitle":true,"inputWidgets":{},"nuid":"88274da6-4924-4bfc-891c-3b9ba2bba271"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["df = spark.read.csv(f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/test.csv\", header='true')\n","display(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Test Storage Account Connectivity","showTitle":true,"inputWidgets":{},"nuid":"45e728bf-8ca2-468b-8fea-177fb25ebdb6"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType\n","\n","schema = StructType([\n","  StructField(\"id\", IntegerType()),\n","  StructField(\"a\", IntegerType()),\n","  StructField(\"b\", StringType()),\n","  StructField(\"c\", FloatType()),\n","])\n","\n","test_df = spark.createDataFrame([[0, 1, \"2\", 3.14], [1, 4, \"5\", 6.28], [2, 7, \"8\", 9.42]], schema=schema)\n","display(test_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Create a Dataframe","showTitle":true,"inputWidgets":{},"nuid":"f39bd460-756b-43f0-aac9-b0787c44ed25"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["sql_pool_url = 'syn-dbw-syn-workshop.sql.azuresynapse.net'\n","serverless_url = 'syn-dbw-syn-workshop-ondemand.sql.azuresynapse.net'\n","user = 'sqladminuser'\n","db_name = 'dbwsynworkshoppool'\n","jdbc_connection_string = f'jdbc:sqlserver://{sql_pool_url}:1433;database={db_name};user={user};password={secrets.sql_pw};encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.database.windows.net;loginTimeout=30;'\n","db_table = 'BatchData'\n","\n","(\n","  test_df.write\n","  .format(\"com.databricks.spark.sqldw\")\n","  .option(\"url\", jdbc_connection_string)\n","  .option(\"forwardSparkAzureStorageCredentials\", \"true\")\n","  .option(\"dbTable\", db_table)\n","  .option(\"tempDir\", f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/\")\n","  .mode(\"overwrite\") # errorifexists, append, overwrite, ignore\n","  .save()\n",")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Write Dataframe to Synapse SQL","showTitle":true,"inputWidgets":{},"nuid":"953d6e5c-2d26-4667-bc3e-980e716925a3"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["# What just happened?\n","\n","The Azure Synapse connector for Databricks leverages Azure Storage and PolyBase / `COPY` to transfer large volumes of data efficiently between Databricks and Synapse. The Azure storage container acts as an intermediary to store bulk data when reading from or writing to Azure Synapse. Spark connects to the storage container using one of the built-in connectors (ADLS or Blob). Once the data is stored in Azure Storage, Synapse connects to the storage account for loading and unloading of that temporary data.\n","\n","## Why is there an Azure Storage container here?\n","\n","The Azure Storage container is used as a temporary staging area for the data to reside before loading into Synapse. This allows better performance for very large datasets as the Data Warehouse can leverage CTAS or `COPY` statements for bulk insertion.\n","\n","## `COPY` vs PolyBase\n","\n","Which of these two methods should I prefer? In general, prefer to use COPY as it provides the most flexibility for high-throughput data ingestion.\n","\n","* Use lower privileged users to load without needing strict CONTROL permissions on the data warehouse\n","* Execute a single T-SQL statement without having to create any additional database objects\n","* Properly parse and load CSV files where delimiters (string, field, row) are escaped within string delimited columns\n","* Specify a finer permission model without exposing storage account keys using Share Access Signatures (SAS)\n","* Use a different storage account for the ERRORFILE location (REJECTED_ROW_LOCATION)\n","* Customize default values for each target column and specify source data fields to load into specific target columns\n","* Specify a custom row terminator for CSV files\n","* Leverage SQL Server Date formats for CSV files\n","* Specify wildcards and multiple files in the storage location path\n","\n","Selection of a specific write method can be configured by setting the `writeSemantics` Spark configuration:\n","\n","```\n","# Configure the write semantics for Azure Synapse connector in the notebook session conf.\n","spark.conf.set(\"spark.databricks.sqldw.writeSemantics\", \"<copy OR polybase>\")\n","```\n","\n","For Databricks 7.0 and above, `COPY` is used by default. Also, the `COPY` semantic is only available on Synapse Gen2 instances.\n","\n","It should be noted that the SQL user permissions differ between the two methods so ensure you've set this up properly in the Synapse DW before switching.\n","\n","## `COPY` Details\n","\n","\n","\n","## PolyBase Details\n","\n","PolyBase is leveraged using a `CREATE TABLE AS SELECT` (CTAS) statement. It's often used for populating staging tables before loading into your final Data Warehouse schema. It's leveraged behind the scenes by the Synapse connector to efficiently populate data in your Data Warehouse.\n","\n","In general, the process for using PolyBase will register a location in Blob Storage as an external table. It will then use the CTAS statement to populate a relational table with the data in the external table.\n","\n","# References\n","\n","PolyBase: https://docs.microsoft.com/en-us/sql/relational-databases/polybase/polybase-guide?view=sql-server-ver15\n","Spark save modes: https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html#save-modes"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d98ca176-289a-4a6d-ab09-5f91105a6f88"}}},{"cell_type":"code","source":["df2 = (\n","  spark.read\n","  .format(\"com.databricks.spark.sqldw\")\n","  .option(\"url\", jdbc_connection_string)\n","  .option(\"forwardSparkAzureStorageCredentials\", \"true\")\n","  .option(\"dbTable\", db_table)\n","  .option(\"tempDir\", f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/\").load()\n",")\n","display(df2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Read back data from Synapse SQL","showTitle":true,"inputWidgets":{},"nuid":"9370d953-de55-4e0b-9766-cac7cc528f87"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["# Streaming\n","\n","The Azure Synapse connector offers efficient and scalable Structured Streaming write support for Azure Synapse that provides consistent user experience with batch writes, and uses PolyBase or COPY for large data transfers between an Azure Databricks cluster and Azure Synapse instance. Similar to the batch writes, streaming is designed largely for ETL, thus providing higher latency that **may not be suitable for real-time data processing in some cases**.\n","\n","## Fault tolerance semantics\n","\n","By default, Azure Synapse Streaming offers **end-to-end exactly-once** guarantee for writing data into an Azure Synapse table by reliably tracking progress of the query using a combination of checkpoint location in DBFS, checkpoint table in Azure Synapse, and locking mechanism to ensure that streaming can handle any types of failures, retries, and query restarts. Optionally, you can select less restrictive at-least-once semantics for Azure Synapse Streaming by setting `spark.databricks.sqldw.streaming.exactlyOnce.enabled` option to `false`, in which case data duplication could occur in the event of intermittent connection failures to Azure Synapse or unexpected query termination."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d330031b-6c00-44bd-87f7-bfbf6d48de1d"}}},{"cell_type":"code","source":["# Prepare streaming source; this could be Kafka or a simple rate stream.\n","streaming_df = (\n","   spark.readStream\n","  .format(\"rate\")\n","  .option(\"rowsPerSecond\", \"100\")\n","  .option(\"numPartitions\", \"16\")\n","  .load()\n",")\n","\n","db_table = 'StreamingData'\n","\n","# Apply some transformations to the data then use\n","# Structured Streaming API to continuously write the data to a table in Azure Synapse.\n","(\n","  streaming_df.writeStream\n","  .format(\"com.databricks.spark.sqldw\")\n","  .option(\"url\", jdbc_connection_string)\n","  .option(\"forwardSparkAzureStorageCredentials\", \"true\")\n","  .option(\"dbTable\", db_table)\n","  .option(\"tempDir\", f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/\")\n","  .option(\"checkpointLocation\", \"/tmp_checkpoint_location\")\n","  .start()\n",")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9cbdf7f4-7e32-4944-ac74-10ff230fdad1"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["## Streaming Differences\n","\n","There are a few difference between the batch write and the streaming write.\n","\n","1. Source\n","\n","  The source of the streaming example is a streaming Dataframe. This could be a variety of difference sources such as an Event Hub or a Kafka topic. In our case, we use a simple `rate` stream that simply outputs a timestamp and a count for testing purposes.\n","\n","2. `writeStream` instead of `write`\n","\n","  In order to set up a structured stream, we use `writeStream` instead of `write`.\n","\n","3. `checkpointLocation`\n","\n","  To support the exactly-once guarantee we leverage a checkpoint location on the Databricks Filesystem (DBFS). This will additionally leverage a checkpoint table in Synapse with a locking mechanism to ensure that the streaming can handle any types of failures, retried and query restarts."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e3ff37fc-9ffa-49e5-b770-59d36b50bcc2"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"2-Synapse-Connector","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":989906665988650}},"nbformat":4,"nbformat_minor":0}