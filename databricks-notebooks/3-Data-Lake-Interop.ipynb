{"cells":[{"cell_type":"markdown","source":["## Part 2: Data Lake Interop\n","\n","Pre-reqs:\n","* Storage account (ADLS Gen 2)\n","* Synapse Serverless Pool\n","* Databricks Service Principal\n","* Secret scope with service principal secret, sql password, and storage account key\n","\n","\n","Overview:\n","\n","1. Setup\n","  * Access secrets\n","  * Configure variables\n","  * Test storage account access\n","2. Write to Data Lake in Delta\n","  * Create a dataframe\n","  * Use delta Spark connector to write to ADLS\n","3. Synapse Read from Delta\n","4. Synapse Write to Delta\n","5. Databricks Read from Delta\n","\n","## Why Delta?\n","\n","Delta Lake is quickly becoming a core format for Data Lake storage in Azure. It combines the best of the parquet format with ACID compliance, time travel, automated compaction and a ton of other features to give you a robust, efficient and cost-effective storage for your organization's data. Additionally, it's becoming more and more supported by native Azure services like Power BI, Azure Analysis Service and (as we will see) Azure Synapse. In most cases, what is being shown below can be performed with other supported formats like parquet, ORC, JSON, CSV, etc."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"415dfc6d-b205-4194-b4d7-83ea18499d77"}}},{"cell_type":"code","source":["from types import SimpleNamespace\n","\n","secret_scope = 'dbw-syn-lab'\n","secrets = SimpleNamespace(\n","  sp_secret = dbutils.secrets.get(secret_scope, 'dbw-syn-lab-sp-secret'),\n","  sa_secret = dbutils.secrets.get(secret_scope, 'dbw-syn-lab-sa-secret'),\n","  sql_pw = dbutils.secrets.get(secret_scope, 'dbw-syn-lab-sql-pw')\n",")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Pull Secrets from Secret Scope","showTitle":true,"inputWidgets":{},"nuid":"05dd1491-b69a-4f83-aadc-171084fe499e"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["The Azure Synapse connector uses three types of network connections:\n","\n","* Spark driver to Azure Synapse\n","* Spark driver and executors to Azure storage account\n","* Azure Synapse to Azure storage account\n","\n","```\n","                                 ┌─────────┐\n","      ┌─────────────────────────>│ STORAGE │<────────────────────────┐\n","      │   Storage acc key /      │ ACCOUNT │  Storage acc key /      │\n","      │   Managed Service ID /   └─────────┘  OAuth 2.0 /            │\n","      │                               │                              │\n","      │                               │                              │\n","      │                               │ Storage acc key /            │\n","      │                               │ OAuth 2.0 /                  │\n","      │                               │                              │\n","      v                               v                       ┌──────v────┐\n","┌──────────┐                      ┌──────────┐                │┌──────────┴┐\n","│ Synapse  │                      │  Spark   │                ││ Spark     │\n","│ Analytics│<────────────────────>│  Driver  │<───────────────>│ Executors │\n","└──────────┘  JDBC with           └──────────┘    Configured   └───────────┘\n","              username & password /                in Spark\n","```\n","\n","It should be noted that use of Blob storage can only used the Storage Account Key, whereas ADLS Gen 2 can optionally use OAuth 2.0 instead."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6e5d163c-504c-49cc-9447-0b63d4115189"}}},{"cell_type":"code","source":["# Application ID corresponds the App Registration / Service Principal used by Databricks\n","app_id = '4b309858-a987-4d5a-9a11-a84116790317'\n","\n","# Directory ID is the tenant this databricks workspace belongs to\n","directory_id = '6871727a-5747-424a-b9d4-39a621930267'\n","\n","# Defining the service principal credentials for the Azure storage account\n","spark.conf.set(\"fs.azure.account.auth.type\", \"OAuth\")\n","spark.conf.set(\"fs.azure.account.oauth.provider.type\",  \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n","spark.conf.set(\"fs.azure.account.oauth2.client.id\", app_id)\n","spark.conf.set(\"fs.azure.account.oauth2.client.secret\", secrets.sp_secret)\n","spark.conf.set(\"fs.azure.account.oauth2.client.endpoint\", f\"https://login.microsoftonline.com/{directory_id}/oauth2/token\")\n","\n","# Defining a separate set of service principal credentials for Azure Synapse Analytics (If not defined, the connector will use the Azure storage account credentials)\n","spark.conf.set(\"spark.databricks.sqldw.jdbc.service.principal.client.id\", app_id)\n","spark.conf.set(\"spark.databricks.sqldw.jdbc.service.principal.client.secret\", secrets.sp_secret)\n","\n","# Setup the storage account key\n","storage_account_name = 'strdbwsynworkshop'\n","container_name = 'synstorage'\n","spark.conf.set(f\"fs.azure.account.key.{storage_account_name}.dfs.core.windows.net\", secrets.sa_secret)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Setup variables and spark configuration","showTitle":true,"inputWidgets":{},"nuid":"88274da6-4924-4bfc-891c-3b9ba2bba271"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["configs = {\n","  \"fs.azure.account.auth.type\": \"OAuth\",\n","  \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n","  \"fs.azure.account.oauth2.client.id\": app_id,\n","  \"fs.azure.account.oauth2.client.secret\": secrets.sp_secret,\n","  \"fs.azure.account.oauth2.client.endpoint\": f\"https://login.microsoftonline.com/{directory_id}/oauth2/token\"\n","}\n","\n","dbutils.fs.unmount(f'/mnt/{container_name}')\n","dbutils.fs.mount(\n","  source = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/\",\n","  mount_point = f'/mnt/{container_name}',\n","  extra_configs = configs\n",")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f22a59b9-2c4e-4881-a4e1-c703ff710654"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["df = spark.read.csv(f\"/mnt/{container_name}/test.csv\", header='true')\n","display(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Test Storage Account Connectivity","showTitle":true,"inputWidgets":{},"nuid":"45e728bf-8ca2-468b-8fea-177fb25ebdb6"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType\n","\n","schema = StructType([\n","  StructField(\"id\", IntegerType()),\n","  StructField(\"a\", IntegerType()),\n","  StructField(\"b\", StringType()),\n","  StructField(\"c\", FloatType()),\n","])\n","\n","test_df = spark.createDataFrame([[0, 1, \"2\", 3.14], [1, 4, \"5\", 6.28], [2, 7, \"8\", 9.42]], schema=schema)\n","display(test_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Create a Dataframe","showTitle":true,"inputWidgets":{},"nuid":"f39bd460-756b-43f0-aac9-b0787c44ed25"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["(\n","  test_df.write\n","  .format(\"delta\")\n","  .save(f\"/mnt/{container_name}/delta_test\")\n",")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Write data as Delta in ADLS","showTitle":true,"inputWidgets":{},"nuid":"fbf315ee-0c52-45b2-89ec-1dd71742f07b"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["We now have an untracked Delta table in our ADLS Gen2 container.\n","\n","## Querying from Synapse Serverless / on-demand\n","\n","We can query Delta directly from Synapse (public preview as of April 2021).\n","\n","```sql\n","SELECT TOP(10) *\n","FROM OPENROWSET(\n","    BULK 'https://strdbwsynworkshop.blob.core.windows.net/synstorage/delta_test',\n","    FORMAT = 'DELTA'\n",") \n","WITH (\n","    id INT,\n","    a INT,\n","    b VARCHAR(6),\n","    c FLOAT\n",") \n","AS rows\n","```\n","\n","## Explanation\n","\n","We can directly select data from our Delta table using Synapse on-demand. This leverages the `OPENROWSET` clause to identify the location and format of our data. Optionally, a schema can be specified using a `WITH` clause which can signifcantly improve performance. This is because we can minimize type sizes (such as using a `VARCHAR(6)` instead of the pessimistic `VARCHAR(1000)`) on our query.\n","\n","## Notable limitations for Serverless queries:\n","\n","* Serverless SQL pools do not support time travel queries or updating Delta Lake files.\n","* Delta Lake support is not available in dedicated SQL pools.\n","* External tables do not support partitioning.\n","* Delta Lake tables created in the Apache Spark pools are not synchronized in serverless SQL pool.\n","* You cannot use schema inference in the OPENROWSET function if you have nested/complex types in the files. Make sure that you explicitly specify the schema in WITH clause.\n","\n","## Reference\n","\n","* https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/query-delta-lake-format#explicitly-specify-schema\n","* https://techcommunity.microsoft.com/t5/azure-synapse-analytics/query-delta-lake-files-using-t-sql-language-in-azure-synapse/ba-p/2388398\n","* Synapse Delta Known Issues: https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/resources-self-help-sql-on-demand#delta-lake"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"749a0d07-e7f5-43b8-b6ec-936cae959795"}}},{"cell_type":"code","source":["# Use the following to clean-up the table\n","# %sh rm -rf /dbfs/mnt/synstorage/delta_test_partitioned_stream && rm -rf /dbfs/delta_test_partitioned_stream_checkpoint\n","\n","from pyspark.sql.functions import expr\n","\n","spark.sql(f'DROP TABLE IF EXISTS events')\n","\n","streaming_delta_location = f'/mnt/{container_name}/delta_test_partitioned_stream/'\n","spark.sql(f\"CREATE TABLE events (timestamp timestamp, value long, partition long) USING DELTA LOCATION '{streaming_delta_location}' PARTITIONED BY (partition)\")\n","\n","# Prepare streaming source; this could be Kafka or a simple rate stream.\n","streaming_df = (\n","   spark.readStream\n","  .format(\"rate\")\n","  .option(\"rowsPerSecond\", \"10\")\n","  .option(\"numPartitions\", \"16\")\n","  .load()\n",")\n","\n","streaming_df = streaming_df.withColumn(\"partition\", expr(\"value % 5\"))\n","\n","# Apply some transformations to the data then use\n","# Structured Streaming API to continuously write the data to a table in Azure Synapse.\n","(\n","  streaming_df.writeStream\n","  .partitionBy(\"partition\")\n","  .outputMode(\"append\")\n","  .option(\"checkpointLocation\", \"/delta_test_partitioned_stream_checkpoint\")\n","  .table(\"events\")\n",")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Stream partitioned data as Delta in ADLS","showTitle":true,"inputWidgets":{},"nuid":"1e9ca70d-f9d2-4185-aae3-2e32e46b24f9"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["df = spark.read.parquet(f\"/mnt/{container_name}/parquet_test\")\n","display(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Read Table written using Synapse CETAS statement","showTitle":true,"inputWidgets":{},"nuid":"53bc09c9-da85-452a-9d81-f10880060a5f"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["# Create a Synapse Delta View\n","\n","Create an external data source"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1909d8aa-a3b8-4b6a-b163-2c8299b40e4b"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"3-Data-Lake-Interop","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":4350319530947815}},"nbformat":4,"nbformat_minor":0}