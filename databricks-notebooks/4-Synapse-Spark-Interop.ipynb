{"cells":[{"cell_type":"markdown","source":["# Part 4 - Synapse Spark\n\nIn this last section, we'll show how you can use Databricks and Synapse's Spark functionality together.\n\nThis is what we'll be building:\n\n```\nDatabricks Write Stream ─────> ADLS ─────> Synapse Read Stream ──────┐\n                                                                     │\n                                                                  Transform\n                                                                     │\nDatabricks Write Stream <────── ADLS <──── Synapse Write Stream ─────┘\n```\n\nWe'll see that structured streams can be setup to seamlessly build workflows using both Databricks and Synapse."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"415dfc6d-b205-4194-b4d7-83ea18499d77"}}},{"cell_type":"code","source":["from types import SimpleNamespace\n\nsecret_scope = 'dbw-syn-lab'\nsecrets = SimpleNamespace(\n  sp_secret = dbutils.secrets.get(secret_scope, 'dbw-syn-lab-sp-secret'),\n  sa_secret = dbutils.secrets.get(secret_scope, 'dbw-syn-lab-sa-secret'),\n  sql_pw = dbutils.secrets.get(secret_scope, 'dbw-syn-lab-sql-pw')\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Pull Secrets from Secret Scope","showTitle":true,"inputWidgets":{},"nuid":"05dd1491-b69a-4f83-aadc-171084fe499e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["The Azure Synapse connector uses three types of network connections:\n\n* Spark driver to Azure Synapse\n* Spark driver and executors to Azure storage account\n* Azure Synapse to Azure storage account\n\n```\n                                 ┌─────────┐\n      ┌─────────────────────────>│ STORAGE │<────────────────────────┐\n      │   Storage acc key /      │ ACCOUNT │  Storage acc key /      │\n      │   Managed Service ID /   └─────────┘  OAuth 2.0 /            │\n      │                               │                              │\n      │                               │                              │\n      │                               │ Storage acc key /            │\n      │                               │ OAuth 2.0 /                  │\n      │                               │                              │\n      v                               v                       ┌──────v────┐\n┌──────────┐                      ┌──────────┐                │┌──────────┴┐\n│ Synapse  │                      │  Spark   │                ││ Spark     │\n│ Analytics│<────────────────────>│  Driver  │<───────────────>│ Executors │\n└──────────┘  JDBC with           └──────────┘    Configured   └───────────┘\n              username & password /                in Spark\n```\n\nIt should be noted that use of Blob storage can only used the Storage Account Key, whereas ADLS Gen 2 can optionally use OAuth 2.0 instead."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6e5d163c-504c-49cc-9447-0b63d4115189"}}},{"cell_type":"code","source":["# Application ID corresponds the App Registration / Service Principal used by Databricks\napp_id = '4b309858-a987-4d5a-9a11-a84116790317'\n\n# Directory ID is the tenant this databricks workspace belongs to\ndirectory_id = '6871727a-5747-424a-b9d4-39a621930267'\n\n# Defining the service principal credentials for the Azure storage account\nspark.conf.set(\"fs.azure.account.auth.type\", \"OAuth\")\nspark.conf.set(\"fs.azure.account.oauth.provider.type\",  \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\nspark.conf.set(\"fs.azure.account.oauth2.client.id\", app_id)\nspark.conf.set(\"fs.azure.account.oauth2.client.secret\", secrets.sp_secret)\nspark.conf.set(\"fs.azure.account.oauth2.client.endpoint\", f\"https://login.microsoftonline.com/{directory_id}/oauth2/token\")\n\n# Defining a separate set of service principal credentials for Azure Synapse Analytics (If not defined, the connector will use the Azure storage account credentials)\nspark.conf.set(\"spark.databricks.sqldw.jdbc.service.principal.client.id\", app_id)\nspark.conf.set(\"spark.databricks.sqldw.jdbc.service.principal.client.secret\", secrets.sp_secret)\n\n# Setup the storage account key\nstorage_account_name = 'strdbwsynworkshop'\ncontainer_name = 'synstorage'\nspark.conf.set(f\"fs.azure.account.key.{storage_account_name}.dfs.core.windows.net\", secrets.sa_secret)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Setup variables and spark configuration","showTitle":true,"inputWidgets":{},"nuid":"88274da6-4924-4bfc-891c-3b9ba2bba271"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["configs = {\n  \"fs.azure.account.auth.type\": \"OAuth\",\n  \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n  \"fs.azure.account.oauth2.client.id\": app_id,\n  \"fs.azure.account.oauth2.client.secret\": secrets.sp_secret,\n  \"fs.azure.account.oauth2.client.endpoint\": f\"https://login.microsoftonline.com/{directory_id}/oauth2/token\"\n}\n\ndbutils.fs.unmount(f'/mnt/{container_name}')\ndbutils.fs.mount(\n  source = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/\",\n  mount_point = f'/mnt/{container_name}',\n  extra_configs = configs\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f22a59b9-2c4e-4881-a4e1-c703ff710654"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["df = spark.read.csv(f\"/mnt/{container_name}/test.csv\", header='true')\ndisplay(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Test Storage Account Connectivity","showTitle":true,"inputWidgets":{},"nuid":"45e728bf-8ca2-468b-8fea-177fb25ebdb6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# Use the following to clean-up the table\n# %sh rm -rf /dbfs/mnt/synstorage/delta_test_partitioned_stream && rm -rf /dbfs/delta_test_partitioned_stream_checkpoint\n\nfrom pyspark.sql.functions import expr\n\n# spark.sql(f'DROP TABLE IF EXISTS events')\nstreaming_delta_location = f'/mnt/{container_name}/delta_test_partitioned_stream/'\n# spark.sql(f\"CREATE TABLE events (timestamp timestamp, value long, partition long) USING DELTA LOCATION '{streaming_delta_location}' PARTITIONED BY (partition)\")\n\n# Prepare streaming source; this could be Kafka or a simple rate stream.\nstreaming_df = (\n   spark.readStream\n  .format(\"rate\")\n  .option(\"rowsPerSecond\", \"10\")\n  .option(\"numPartitions\", \"16\")\n  .load()\n)\n\nstreaming_df = streaming_df.withColumn(\"partition\", expr(\"value % 5\"))\n\n# Apply some transformations to the data then use\n# Structured Streaming API to continuously write the data to a table in Azure Synapse.\n(\n  streaming_df.writeStream\n  .partitionBy(\"partition\")\n  .outputMode(\"append\")\n  .option(\"checkpointLocation\", \"/delta_test_partitioned_stream_checkpoint\")\n  .table(\"events\")\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Stream partitioned data as Delta in ADLS","showTitle":true,"inputWidgets":{},"nuid":"1e9ca70d-f9d2-4185-aae3-2e32e46b24f9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# Once the Synapse spark stream has been started, you can execute this to see the transformed data in Databricks.\nstreaming_df = (\n   spark.readStream\n  .format(\"delta\")\n  .load(f'/mnt/{container_name}/delta_test_partitioned_stream_synapse/')\n)\ndisplay(streaming_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Read Delta stream from Synapse using Databricks","showTitle":true,"inputWidgets":{},"nuid":"7c8f3825-b622-45fa-aa05-9a7546a57388"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# Example 1: Show how a delta format dataset can be loaded and displayed\ncontainer_name = 'synstorage'\ndf = spark.read.format('delta').load(f'/mnt/{container_name}/delta_test_partitioned_stream_synapse/')\ndisplay(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"afffee6f-13d3-4ccd-bd90-6b4e4b515bda"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"4-Synapse-Spark-Interop","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":84367878471274}},"nbformat":4,"nbformat_minor":0}
